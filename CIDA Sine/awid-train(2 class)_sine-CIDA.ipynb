{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install / upgrade necessary packages\n",
    "# !pip install easydict\n",
    "# !pip install numpy\n",
    "# !pip install numpy --upgrade # upgrade numpy to at least 1.19\n",
    "# !pip install torch --upgrade # upgrade pytorch to 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict()\n",
    "\n",
    "args.epochs=11\n",
    "args.dropout=0.0\n",
    "args.lr=5e-5\n",
    "args.gamma_exp=1000\n",
    "args.hidden=800\n",
    "args.ratio=1\n",
    "args.dis_lambda=1.0\n",
    "args.lambda_m=0.0\n",
    "args.wgan='wgan'\n",
    "args.clamp_lower=-0.15\n",
    "args.clamp_upper=0.15\n",
    "args.batch_size=100\n",
    "args.num_train=100\n",
    "args.loss='default'\n",
    "args.evaluate=False\n",
    "args.checkpoint='none'\n",
    "args.save_head='tmp'\n",
    "args.save_interval=20\n",
    "args.log_interval=20\n",
    "args.log_file='tmp_mlp'\n",
    "args.seed=2 #2\n",
    "args.cuda=False\n",
    "args.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.18.5\n",
      "Pytorch version: 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avin/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/avin/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:153: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from utils import plain_log\n",
    "from utils import write_pickle,read_pickle\n",
    "from utils import masked_cross_entropy\n",
    "from utils import gaussian_loss\n",
    "from torch.utils import data\n",
    "from data_loader import toydata\n",
    "import pandas as pd\n",
    "\n",
    "label_noise_std = 0.50\n",
    "use_label_noise = False\n",
    "use_inverse_weighted = True\n",
    "discr_thres = 999.999\n",
    "normalize = True\n",
    "train_discr_step_tot = 2\n",
    "train_discr_step_extra = 0\n",
    "slow_lrD_decay = 1\n",
    "norm = 8\n",
    "fname_save = 'pred_tmp.pkl'\n",
    "fname = '/Users/avin/Desktop/Intrusion Detection/Datasets/awid.pkl'\n",
    "fname_test='/Users/avin/Desktop/Intrusion Detection/Datasets/awid.pkl'\n",
    "data = read_pickle(fname_test)\n",
    "true_class=data['label']\n",
    "train_list = list(range(2))\n",
    "mask_list = [1] + [0] \n",
    "test_list = list(range(2))\n",
    "cm= False\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(int(args.seed))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname, train_list, normalize, mask_list),\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname_test, test_list, normalize),\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pytorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data and plot it for visualization only\n",
    "#from plot import plot_dataset\n",
    "#import matplotlib.pyplot as plt\n",
    "#data_pkl = read_pickle(f'./awid.pkl')\n",
    "#plot_dataset(data_pkl)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: domain as input to generate feature, and then concatenate, deep dynamic layers\n",
    "class DomainEnc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainEnc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.ratio = float(args.ratio)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.fc1 = nn.Linear(153, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop3 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop4 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden)\n",
    "\n",
    "        self.fc1_var = nn.Linear(1, int(self.hidden // self.ratio))\n",
    "        self.fc2_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.fc3_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.drop1_var = nn.Dropout(self.dropout)\n",
    "        self.drop2_var = nn.Dropout(self.dropout)\n",
    "        self.drop3_var = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        # side branch for variable FC\n",
    "        x_domain = F.relu(self.fc1_var(domain))\n",
    "        x_domain = self.drop1_var(x_domain)\n",
    "\n",
    "        # main branch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # combine feature in the middle\n",
    "        x = torch.cat((x, x_domain), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        # continue main branch\n",
    "        x = F.relu(self.fc_final(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor\n",
    "class DomainPred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainPred, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop0 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden, 153)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop0(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc_final(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator: with BN layers after each FC, dual output\n",
    "class DomainDDisc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainDDisc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final_m = nn.Linear(self.hidden, 1)\n",
    "        self.fc_final_s = nn.Linear(self.hidden, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x_m = F.relu(self.bn3_m(self.fc3_m(x)))\n",
    "        x_m = self.drop3_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn3_s(self.fc3_s(x)))\n",
    "        x_s = self.drop3_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn4_m(self.fc4_m(x_m)))\n",
    "        x_m = self.drop4_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn4_s(self.fc4_s(x_s)))\n",
    "        x_s = self.drop4_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn5_m(self.fc5_m(x_m)))\n",
    "        x_m = self.drop5_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn5_s(self.fc5_s(x_s)))\n",
    "        x_s = self.drop5_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn6_m(self.fc6_m(x_m)))\n",
    "        x_m = self.drop6_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn6_s(self.fc6_s(x_s)))\n",
    "        x_s = self.drop6_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn7_m(self.fc7_m(x_m)))\n",
    "        x_m = self.drop7_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn7_s(self.fc7_s(x_s)))\n",
    "        x_s = self.drop7_s(x_s)\n",
    "\n",
    "        x_m = self.fc_final_m(x_m)\n",
    "        x_s = self.fc_final_s(x_s) # log sigma^2\n",
    "\n",
    "        return (x_m, x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "encoder = DomainEnc()\n",
    "predictor = DomainPred()\n",
    "discriminator = DomainDDisc()\n",
    "models = [encoder, predictor, discriminator]\n",
    "if args.cuda:\n",
    "    for model in models:\n",
    "        model.cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: avg_discr_loss = 0.05417, avg_pred_loss = 5497918566.400, avg_total_loss = 5497918566.400,\n",
      "Test set: Average loss: 3384902462.12, Accuracy: 2359/3000 (78.63%), Precision: 0.83%, Recall: 0.79%, f1: 0.78%\n",
      "Train Epoch 2: avg_discr_loss = -0.09936, avg_pred_loss = 2275094240.000, avg_total_loss = 2275094240.000,\n",
      "Test set: Average loss: 2259282438.83, Accuracy: 2575/3000 (85.83%), Precision: 0.88%, Recall: 0.86%, f1: 0.86%\n",
      "Train Epoch 3: avg_discr_loss = -0.20874, avg_pred_loss = 1523757864.533, avg_total_loss = 1523757864.533,\n",
      "Test set: Average loss: 786645886.29, Accuracy: 2743/3000 (91.43%), Precision: 0.92%, Recall: 0.91%, f1: 0.91%\n",
      "Train Epoch 4: avg_discr_loss = -0.31047, avg_pred_loss = 1135570520.267, avg_total_loss = 1135570520.267,\n",
      "Test set: Average loss: 793660798.29, Accuracy: 2805/3000 (93.50%), Precision: 0.94%, Recall: 0.94%, f1: 0.93%\n",
      "Train Epoch 5: avg_discr_loss = -0.40543, avg_pred_loss = 1310628496.000, avg_total_loss = 1310628496.000,\n",
      "Test set: Average loss: 1796859816.62, Accuracy: 2652/3000 (88.40%), Precision: 0.88%, Recall: 0.88%, f1: 0.88%\n",
      "Train Epoch 6: avg_discr_loss = -0.50250, avg_pred_loss = 1241009707.733, avg_total_loss = 1241009707.733,\n",
      "Test set: Average loss: 1015416545.28, Accuracy: 2804/3000 (93.47%), Precision: 0.94%, Recall: 0.93%, f1: 0.93%\n",
      "Train Epoch 7: avg_discr_loss = -0.60579, avg_pred_loss = 812658305.600, avg_total_loss = 812658305.600,\n",
      "Test set: Average loss: 850076953.26, Accuracy: 2778/3000 (92.60%), Precision: 0.93%, Recall: 0.93%, f1: 0.93%\n",
      "Train Epoch 8: avg_discr_loss = -0.69942, avg_pred_loss = 1276611608.267, avg_total_loss = 1276611608.267,\n",
      "Test set: Average loss: 1038820037.97, Accuracy: 2775/3000 (92.50%), Precision: 0.93%, Recall: 0.93%, f1: 0.92%\n",
      "Train Epoch 9: avg_discr_loss = -0.78189, avg_pred_loss = 1262310467.733, avg_total_loss = 1262310467.733,\n",
      "Test set: Average loss: 397648776.53, Accuracy: 2846/3000 (94.87%), Precision: 0.95%, Recall: 0.95%, f1: 0.95%\n",
      "Train Epoch 10: avg_discr_loss = -0.84705, avg_pred_loss = 508569905.333, avg_total_loss = 508569905.359,\n",
      "Test set: Average loss: 936324024.32, Accuracy: 2315/3000 (77.17%), Precision: 0.83%, Recall: 0.77%, f1: 0.76%\n",
      "Train Epoch 11: avg_discr_loss = -0.88696, avg_pred_loss = 516999054.133, avg_total_loss = 516999054.133,\n",
      "Test set: Average loss: 307072849.24, Accuracy: 2847/3000 (94.90%), Precision: 0.95%, Recall: 0.95%, f1: 0.95%\n",
      "Test set: Average loss: 307072849.24, Accuracy: 2847/3000 (94.90%), Precision: 0.95%, Recall: 0.95%, f1: 0.95%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set up optimizer\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "\n",
    "ind = list(range(args.batch_size))\n",
    "ind_test = list(range(1000))\n",
    "bce = nn.BCELoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    sum_discr_loss = 0\n",
    "    sum_total_loss = 0\n",
    "    sum_pred_loss = 0\n",
    "    for batch_idx, data_tuple in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data, mask = data_tuple\n",
    "        else:\n",
    "            data, target, domain, mask = data_tuple\n",
    "\n",
    "        # FF encoder and predictor\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "\n",
    "        if use_label_noise:\n",
    "            noise = (torch.randn(domain.size()).cuda() * label_noise_std).unsqueeze(1)\n",
    "\n",
    "        # train discriminator\n",
    "        train_discr_step = 0\n",
    "        while args.dis_lambda > 0.0:\n",
    "            train_discr_step += 1\n",
    "            discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "            discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm, np.mean(train_list) / norm, norm)\n",
    "            for model in models:\n",
    "                model.zero_grad()\n",
    "            discr_loss.backward(retain_graph=True)\n",
    "            opt_D.step()\n",
    "\n",
    "            # handle extra steps to train the discr's variance branch\n",
    "            if train_discr_step_extra > 0:\n",
    "                cur_extra_step = 0\n",
    "                while True:\n",
    "                    discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "                    discr_loss = gaussian_loss(discr_pred_m.detach(), discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "                    for model in models:\n",
    "                        model.zero_grad()\n",
    "                    discr_loss.backward(retain_graph=True)\n",
    "                    opt_D.step()\n",
    "                    cur_extra_step += 1\n",
    "                    if cur_extra_step > train_discr_step_extra:\n",
    "                        break\n",
    "\n",
    "            if discr_loss.item() < 1.1 * discr_thres and train_discr_step >= train_discr_step_tot:\n",
    "                sum_discr_loss += discr_loss.item()\n",
    "                break\n",
    "\n",
    "        # handle wgan\n",
    "        if args.wgan == 'wgan':\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(args.clamp_lower, args.clamp_upper)\n",
    "\n",
    "        # train encoder and predictor\n",
    "        pred_loss = masked_cross_entropy(prediction, target, mask)\n",
    "        discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "        ent_loss = 0\n",
    "\n",
    "        discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "        total_loss = pred_loss - discr_loss * args.dis_lambda\n",
    "\n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt_non_D.step()\n",
    "        sum_pred_loss += pred_loss.item()\n",
    "        sum_total_loss += total_loss.item()\n",
    "\n",
    "    lr_scheduler_D.step()\n",
    "    lr_scheduler_non_D.step()\n",
    "\n",
    "    avg_discr_loss = sum_discr_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_pred_loss = sum_pred_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_total_loss = sum_total_loss / len(train_loader.dataset) * args.batch_size\n",
    "    log_txt = 'Train Epoch {}: avg_discr_loss = {:.5f}, avg_pred_loss = {:.3f}, avg_total_loss = {:.3f},'.format(epoch, avg_discr_loss, avg_pred_loss, avg_total_loss)\n",
    "    print(log_txt)\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "    if epoch % args.save_interval == 0 and epoch != 0:\n",
    "        torch.save(encoder, '%s.model_enc' % args.save_head)\n",
    "        torch.save(predictor, '%s.model_pred' % args.save_head)\n",
    "        torch.save(discriminator, '%s.model_discr' % args.save_head)\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    test_loss = 0\n",
    "    rmse_loss = 0\n",
    "    correct = 0\n",
    "    l_data = []\n",
    "    l_label = []\n",
    "    l_gt = []\n",
    "    l_encoding = []\n",
    "    l_domain = []\n",
    "    l_prob = []\n",
    "    #for data, target, domain in test_loader:\n",
    "    for data_tuple in test_loader:\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data = data_tuple\n",
    "        else:\n",
    "            data, target, domain = data_tuple\n",
    "            data_raw = data\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "        test_loss += F.nll_loss(prediction, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = prediction.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "##\n",
    "        l_data.append(data_raw.cpu().numpy())\n",
    "        l_label.append(pred.cpu().numpy())\n",
    "        l_gt.append(target.cpu().numpy())\n",
    "        l_encoding.append(encoding.data.cpu().numpy())\n",
    "        l_domain.append(domain.data.cpu().numpy())\n",
    "        l_prob.append(prediction.data.cpu().numpy())\n",
    "\n",
    "    data_all = np.concatenate(l_data, axis=0)\n",
    "    label_all = np.concatenate(l_label, axis=0)\n",
    "    gt_all = np.concatenate(l_gt, axis=0)\n",
    "    encoding_all = np.concatenate(l_encoding, axis=0)\n",
    "    domain_all = np.concatenate(l_domain, axis=0)\n",
    "    prob_all = np.concatenate(l_prob, axis=0)\n",
    "    d_pkl = dict()\n",
    "    d_pkl['data'] = data_all\n",
    "    d_pkl['label'] = label_all[:, 0]\n",
    "    d_pkl['gt'] = gt_all\n",
    "    d_pkl['encoding'] = encoding_all\n",
    "    d_pkl['domain'] = domain_all\n",
    "    d_pkl['prob'] = prob_all\n",
    "    write_pickle(d_pkl, fname_save)\n",
    "    \n",
    "    p=np.asarray(label_all)\n",
    "    x=pd.DataFrame(p.flatten(),columns = ['col1'])\n",
    "    predicted_class=x['col1'].to_numpy()\n",
    "\n",
    "    pre= precision_score(true_class, predicted_class, average= 'macro')\n",
    "    recall= recall_score(true_class, predicted_class, average= 'macro')\n",
    "    f1= f1_score(true_class, predicted_class, average= 'macro')\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    log_txt = 'Test set: Average loss: {:.2f}, Accuracy: {}/{} ({:.2f}%), Precision: {:.2f}%, Recall: {:.2f}%, f1: {:.2f}%'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset), pre, recall, f1)\n",
    "    print(log_txt)\n",
    "    if(cm==True):\n",
    "        conf=confusion_matrix(true_class,predicted_class)\n",
    "        ax= plt.subplot()\n",
    "        sns.heatmap(conf, annot=True, ax = ax, fmt=\".1f\")\n",
    "        ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "        ax.set_title('Confusion Matrix'); \n",
    "        ax.xaxis.set_ticklabels([ 'Normal', 'Attack']); ax.yaxis.set_ticklabels(['Normal', 'Attack']);\n",
    "        plt.show()\n",
    "\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "\n",
    "if args.checkpoint != 'none':\n",
    "    encoder = torch.load(args.checkpoint + '_enc')\n",
    "    predictor = torch.load(args.checkpoint + '_pred')\n",
    "    discriminator = torch.load(args.checkpoint + '_discr')\n",
    "    opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "    opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "    lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "    lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "    models = [encoder, predictor, discriminator]\n",
    "    for model in models:\n",
    "        for key, module in model._modules.items():\n",
    "            print('key', key)\n",
    "            print('module', module)\n",
    "\n",
    "if not args.evaluate:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch % 1 == 0:\n",
    "            test()\n",
    "cm=True\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
