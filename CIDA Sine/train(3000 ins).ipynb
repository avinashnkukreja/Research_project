{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict()\n",
    "\n",
    "args.epochs=50\n",
    "args.dropout=0.0\n",
    "args.lr=5e-5\n",
    "args.gamma_exp=1000\n",
    "args.hidden=800\n",
    "args.ratio=1\n",
    "args.dis_lambda=1.0\n",
    "args.lambda_m=0.0\n",
    "args.wgan='wgan'\n",
    "args.clamp_lower=-0.15\n",
    "args.clamp_upper=0.15\n",
    "args.batch_size=100\n",
    "args.num_train=100\n",
    "args.loss='default'\n",
    "args.evaluate=False\n",
    "args.checkpoint='none'\n",
    "args.save_head='tmp'\n",
    "args.save_interval=20\n",
    "args.log_interval=20\n",
    "args.log_file='tmp_mlp'\n",
    "args.seed=2 #2\n",
    "args.cuda=True\n",
    "args.device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.19.2\n",
      "Pytorch version: 1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from utils import plain_log\n",
    "from utils import write_pickle,read_pickle\n",
    "from utils import masked_cross_entropy\n",
    "from utils import gaussian_loss\n",
    "from torch.utils import data\n",
    "from data_loader import toydata\n",
    "import pandas as pd\n",
    "\n",
    "label_noise_std = 0.50\n",
    "use_label_noise = False\n",
    "use_inverse_weighted = True\n",
    "discr_thres = 999.999\n",
    "normalize = True\n",
    "train_discr_step_tot = 2\n",
    "train_discr_step_extra = 0\n",
    "slow_lrD_decay = 1\n",
    "norm = 8\n",
    "fname_save = 'pred_tmp.pkl'\n",
    "fname = 'awid_full_3_4class.pkl'\n",
    "fname_test='awid_full_3_4class.pkl'\n",
    "data = read_pickle(fname_test)\n",
    "true_class=data['label']\n",
    "train_list = list(range(2))\n",
    "mask_list = [1] + [0] \n",
    "test_list = list(range(2))\n",
    "cm= False\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(int(args.seed))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname, train_list, normalize, mask_list),\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname_test, test_list, normalize),\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pytorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: domain as input to generate feature, and then concatenate, deep dynamic layers\n",
    "class DomainEnc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainEnc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.ratio = float(args.ratio)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.fc1 = nn.Linear(153, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop3 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop4 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden)\n",
    "\n",
    "        self.fc1_var = nn.Linear(1, int(self.hidden // self.ratio))\n",
    "        self.fc2_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.fc3_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.drop1_var = nn.Dropout(self.dropout)\n",
    "        self.drop2_var = nn.Dropout(self.dropout)\n",
    "        self.drop3_var = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        # side branch for variable FC\n",
    "        x_domain = F.relu(self.fc1_var(domain))\n",
    "        x_domain = self.drop1_var(x_domain)\n",
    "\n",
    "        # main branch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # combine feature in the middle\n",
    "        x = torch.cat((x, x_domain), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        # continue main branch\n",
    "        x = F.relu(self.fc_final(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor\n",
    "class DomainPred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainPred, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop0 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden, 153)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop0(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc_final(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator: with BN layers after each FC, dual output\n",
    "class DomainDDisc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainDDisc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final_m = nn.Linear(self.hidden, 1)\n",
    "        self.fc_final_s = nn.Linear(self.hidden, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x_m = F.relu(self.bn3_m(self.fc3_m(x)))\n",
    "        x_m = self.drop3_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn3_s(self.fc3_s(x)))\n",
    "        x_s = self.drop3_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn4_m(self.fc4_m(x_m)))\n",
    "        x_m = self.drop4_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn4_s(self.fc4_s(x_s)))\n",
    "        x_s = self.drop4_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn5_m(self.fc5_m(x_m)))\n",
    "        x_m = self.drop5_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn5_s(self.fc5_s(x_s)))\n",
    "        x_s = self.drop5_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn6_m(self.fc6_m(x_m)))\n",
    "        x_m = self.drop6_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn6_s(self.fc6_s(x_s)))\n",
    "        x_s = self.drop6_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn7_m(self.fc7_m(x_m)))\n",
    "        x_m = self.drop7_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn7_s(self.fc7_s(x_s)))\n",
    "        x_s = self.drop7_s(x_s)\n",
    "\n",
    "        x_m = self.fc_final_m(x_m)\n",
    "        x_s = self.fc_final_s(x_s) # log sigma^2\n",
    "\n",
    "        return (x_m, x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "encoder = DomainEnc()\n",
    "predictor = DomainPred()\n",
    "discriminator = DomainDDisc()\n",
    "models = [encoder, predictor, discriminator]\n",
    "if args.cuda:\n",
    "    for model in models:\n",
    "        model.cuda()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: avg_discr_loss = -0.91804, avg_pred_loss = 1771607.092, avg_total_loss = 1771608.099,\n",
      "Test set: Average loss: 118465.34, Accuracy: 267263/300000 (89.09%), Precision: 0.89%, Recall: 0.86%, f1: 0.87%\n",
      "Train Epoch 2: avg_discr_loss = -0.91806, avg_pred_loss = 729090.493, avg_total_loss = 729091.513,\n",
      "Test set: Average loss: 430042.55, Accuracy: 270424/300000 (90.14%), Precision: 0.90%, Recall: 0.88%, f1: 0.89%\n",
      "Train Epoch 3: avg_discr_loss = -0.91815, avg_pred_loss = 584460.881, avg_total_loss = 584461.900,\n",
      "Test set: Average loss: 50036.94, Accuracy: 276817/300000 (92.27%), Precision: 0.73%, Recall: 0.74%, f1: 0.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: avg_discr_loss = -0.91803, avg_pred_loss = 74322.064, avg_total_loss = 74323.086,\n",
      "Test set: Average loss: 4437.07, Accuracy: 151976/300000 (50.66%), Precision: 0.81%, Recall: 0.63%, f1: 0.56%\n",
      "Train Epoch 5: avg_discr_loss = -0.91812, avg_pred_loss = 1860.534, avg_total_loss = 1861.556,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.23, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 6: avg_discr_loss = -0.91817, avg_pred_loss = 6525.868, avg_total_loss = 6526.890,\n",
      "Test set: Average loss: 1.21, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: avg_discr_loss = -0.91818, avg_pred_loss = 1.204, avg_total_loss = 2.226,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 8: avg_discr_loss = -0.91818, avg_pred_loss = 2220.289, avg_total_loss = 2221.311,\n",
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: avg_discr_loss = -0.91820, avg_pred_loss = 88.188, avg_total_loss = 89.210,\n",
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: avg_discr_loss = -0.91820, avg_pred_loss = 1.203, avg_total_loss = 2.225,\n",
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: avg_discr_loss = -0.91821, avg_pred_loss = 9281.053, avg_total_loss = 9282.076,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 12: avg_discr_loss = -0.91822, avg_pred_loss = 767.285, avg_total_loss = 768.308,\n",
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: avg_discr_loss = -0.91822, avg_pred_loss = 1.203, avg_total_loss = 2.225,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.20, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 14: avg_discr_loss = -0.91822, avg_pred_loss = 1.203, avg_total_loss = 2.225,\n",
      "Test set: Average loss: 14.50, Accuracy: 149999/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 15: avg_discr_loss = -0.91822, avg_pred_loss = 1932.839, avg_total_loss = 1933.861,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.21, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 16: avg_discr_loss = -0.91821, avg_pred_loss = 1.230, avg_total_loss = 2.252,\n",
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 17: avg_discr_loss = -0.91821, avg_pred_loss = 1.219, avg_total_loss = 2.241,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.21, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 18: avg_discr_loss = -0.91819, avg_pred_loss = 55372.395, avg_total_loss = 55373.416,\n",
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 19: avg_discr_loss = -0.91819, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n",
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 20: avg_discr_loss = -0.91818, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 21: avg_discr_loss = -0.91818, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 22: avg_discr_loss = -0.91819, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n",
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 23: avg_discr_loss = -0.91818, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n",
      "Train Epoch 24: avg_discr_loss = -0.91819, avg_pred_loss = 1.242, avg_total_loss = 2.264,\n",
      "Test set: Average loss: 1.24, Accuracy: 150000/300000 (50.00%), Precision: 0.12%, Recall: 0.25%, f1: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PLS0144/avinashnkukreja/.conda/envs/im/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set up optimizer\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "\n",
    "ind = list(range(args.batch_size))\n",
    "ind_test = list(range(1000))\n",
    "bce = nn.BCELoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    sum_discr_loss = 0\n",
    "    sum_total_loss = 0\n",
    "    sum_pred_loss = 0\n",
    "    for batch_idx, data_tuple in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data, mask = data_tuple\n",
    "        else:\n",
    "            data, target, domain, mask = data_tuple\n",
    "\n",
    "        # FF encoder and predictor\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "\n",
    "        if use_label_noise:\n",
    "            noise = (torch.randn(domain.size()).cuda() * label_noise_std).unsqueeze(1)\n",
    "\n",
    "        # train discriminator\n",
    "        train_discr_step = 0\n",
    "        while args.dis_lambda > 0.0:\n",
    "            train_discr_step += 1\n",
    "            discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "            discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm, np.mean(train_list) / norm, norm)\n",
    "            for model in models:\n",
    "                model.zero_grad()\n",
    "            discr_loss.backward(retain_graph=True)\n",
    "            opt_D.step()\n",
    "\n",
    "            # handle extra steps to train the discr's variance branch\n",
    "            if train_discr_step_extra > 0:\n",
    "                cur_extra_step = 0\n",
    "                while True:\n",
    "                    discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "                    discr_loss = gaussian_loss(discr_pred_m.detach(), discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "                    for model in models:\n",
    "                        model.zero_grad()\n",
    "                    discr_loss.backward(retain_graph=True)\n",
    "                    opt_D.step()\n",
    "                    cur_extra_step += 1\n",
    "                    if cur_extra_step > train_discr_step_extra:\n",
    "                        break\n",
    "\n",
    "            if discr_loss.item() < 1.1 * discr_thres and train_discr_step >= train_discr_step_tot:\n",
    "                sum_discr_loss += discr_loss.item()\n",
    "                break\n",
    "\n",
    "        # handle wgan\n",
    "        if args.wgan == 'wgan':\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(args.clamp_lower, args.clamp_upper)\n",
    "\n",
    "        # train encoder and predictor\n",
    "        pred_loss = masked_cross_entropy(prediction, target, mask)\n",
    "        discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "        ent_loss = 0\n",
    "\n",
    "        discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "        total_loss = pred_loss - discr_loss * args.dis_lambda\n",
    "\n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt_non_D.step()\n",
    "        sum_pred_loss += pred_loss.item()\n",
    "        sum_total_loss += total_loss.item()\n",
    "\n",
    "    lr_scheduler_D.step()\n",
    "    lr_scheduler_non_D.step()\n",
    "\n",
    "    avg_discr_loss = sum_discr_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_pred_loss = sum_pred_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_total_loss = sum_total_loss / len(train_loader.dataset) * args.batch_size\n",
    "    log_txt = 'Train Epoch {}: avg_discr_loss = {:.5f}, avg_pred_loss = {:.3f}, avg_total_loss = {:.3f},'.format(epoch, avg_discr_loss, avg_pred_loss, avg_total_loss)\n",
    "    print(log_txt)\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "    if epoch % args.save_interval == 0 and epoch != 0:\n",
    "        torch.save(encoder, '%s.model_enc' % args.save_head)\n",
    "        torch.save(predictor, '%s.model_pred' % args.save_head)\n",
    "        torch.save(discriminator, '%s.model_discr' % args.save_head)\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    test_loss = 0\n",
    "    rmse_loss = 0\n",
    "    correct = 0\n",
    "    l_data = []\n",
    "    l_label = []\n",
    "    l_gt = []\n",
    "    l_encoding = []\n",
    "    l_domain = []\n",
    "    l_prob = []\n",
    "    #for data, target, domain in test_loader:\n",
    "    for data_tuple in test_loader:\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data = data_tuple\n",
    "        else:\n",
    "            data, target, domain = data_tuple\n",
    "            data_raw = data\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "        test_loss += F.nll_loss(prediction, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = prediction.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "##\n",
    "        l_data.append(data_raw.cpu().numpy())\n",
    "        l_label.append(pred.cpu().numpy())\n",
    "        l_gt.append(target.cpu().numpy())\n",
    "        l_encoding.append(encoding.data.cpu().numpy())\n",
    "        l_domain.append(domain.data.cpu().numpy())\n",
    "        l_prob.append(prediction.data.cpu().numpy())\n",
    "\n",
    "    data_all = np.concatenate(l_data, axis=0)\n",
    "    label_all = np.concatenate(l_label, axis=0)\n",
    "    gt_all = np.concatenate(l_gt, axis=0)\n",
    "    encoding_all = np.concatenate(l_encoding, axis=0)\n",
    "    domain_all = np.concatenate(l_domain, axis=0)\n",
    "    prob_all = np.concatenate(l_prob, axis=0)\n",
    "    d_pkl = dict()\n",
    "    d_pkl['data'] = data_all\n",
    "    d_pkl['label'] = label_all[:, 0]\n",
    "    d_pkl['gt'] = gt_all\n",
    "    d_pkl['encoding'] = encoding_all\n",
    "    d_pkl['domain'] = domain_all\n",
    "    d_pkl['prob'] = prob_all\n",
    "    write_pickle(d_pkl, fname_save)\n",
    "    \n",
    "    p=np.asarray(label_all)\n",
    "    x=pd.DataFrame(p.flatten(),columns = ['col1'])\n",
    "    predicted_class=x['col1'].to_numpy()\n",
    "\n",
    "    pre= precision_score(true_class, predicted_class, average= 'macro')\n",
    "    recall= recall_score(true_class, predicted_class, average= 'macro')\n",
    "    f1= f1_score(true_class, predicted_class, average= 'macro')\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    log_txt = 'Test set: Average loss: {:.2f}, Accuracy: {}/{} ({:.2f}%), Precision: {:.2f}%, Recall: {:.2f}%, f1: {:.2f}%'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset), pre, recall, f1)\n",
    "    print(log_txt)\n",
    "    if(cm==True):\n",
    "        conf=confusion_matrix(true_class,predicted_class)\n",
    "        ax= plt.subplot()\n",
    "        sns.heatmap(conf, annot=True, ax = ax, fmt=\".1f\")\n",
    "        ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "        ax.set_title('Confusion Matrix'); \n",
    "        ax.xaxis.set_ticklabels(['Flooding', 'Impersination', 'Injection', 'Normal']); ax.yaxis.set_ticklabels(['Flooding', 'Impersination', 'Injection', 'Normal']);\n",
    "        plt.show()\n",
    "\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "\n",
    "if args.checkpoint != 'none':\n",
    "    encoder = torch.load(args.checkpoint + '_enc')\n",
    "    predictor = torch.load(args.checkpoint + '_pred')\n",
    "    discriminator = torch.load(args.checkpoint + '_discr')\n",
    "    opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "    opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "    lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "    lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "    models = [encoder, predictor, discriminator]\n",
    "    for model in models:\n",
    "        for key, module in model._modules.items():\n",
    "            print('key', key)\n",
    "            print('module', module)\n",
    "\n",
    "if not args.evaluate:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch % 1 == 0:\n",
    "            test()\n",
    "        \n",
    "cm=True\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b355d643d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfname_test_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'awid_test_full_4class.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_test_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrue_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m test_loader = torch.utils.data.DataLoader(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pickle' is not defined"
     ]
    }
   ],
   "source": [
    "fname_test_full = 'awid_test_full_4class.pkl'\n",
    "data = read_pickle(fname_test_full)\n",
    "true_class=data['label']\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname_test_full, test_list, normalize),\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "def test():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    test_loss = 0\n",
    "    rmse_loss = 0\n",
    "    correct = 0\n",
    "    l_data = []\n",
    "    l_label = []\n",
    "    l_gt = []\n",
    "    l_encoding = []\n",
    "    l_domain = []\n",
    "    l_prob = []\n",
    "    #for data, target, domain in test_loader:\n",
    "    for data_tuple in test_loader:\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data = data_tuple\n",
    "        else:\n",
    "            data, target, domain = data_tuple\n",
    "            data_raw = data\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "        test_loss += F.nll_loss(prediction, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = prediction.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "##\n",
    "        l_data.append(data_raw.cpu().numpy())\n",
    "        l_label.append(pred.cpu().numpy())\n",
    "        l_gt.append(target.cpu().numpy())\n",
    "        l_encoding.append(encoding.data.cpu().numpy())\n",
    "        l_domain.append(domain.data.cpu().numpy())\n",
    "        l_prob.append(prediction.data.cpu().numpy())\n",
    "\n",
    "    data_all = np.concatenate(l_data, axis=0)\n",
    "    label_all = np.concatenate(l_label, axis=0)\n",
    "    gt_all = np.concatenate(l_gt, axis=0)\n",
    "    encoding_all = np.concatenate(l_encoding, axis=0)\n",
    "    domain_all = np.concatenate(l_domain, axis=0)\n",
    "    prob_all = np.concatenate(l_prob, axis=0)\n",
    "    d_pkl = dict()\n",
    "    d_pkl['data'] = data_all\n",
    "    d_pkl['label'] = label_all[:, 0]\n",
    "    d_pkl['gt'] = gt_all\n",
    "    d_pkl['encoding'] = encoding_all\n",
    "    d_pkl['domain'] = domain_all\n",
    "    d_pkl['prob'] = prob_all\n",
    "    write_pickle(d_pkl, fname_save)\n",
    "    \n",
    "    p=np.asarray(label_all)\n",
    "    x=pd.DataFrame(p.flatten(),columns = ['col1'])\n",
    "    predicted_class=x['col1'].to_numpy()\n",
    "\n",
    "    pre= precision_score(true_class, predicted_class, average= 'macro')\n",
    "    recall= recall_score(true_class, predicted_class, average= 'macro')\n",
    "    f1= f1_score(true_class, predicted_class, average= 'macro')\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    log_txt = 'Test set: Average loss: {:.2f}, Accuracy: {}/{} ({:.2f}%), Precision: {:.2f}%, Recall: {:.2f}%, f1: {:.2f}%'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset), pre, recall, f1)\n",
    "    print(log_txt)\n",
    "    conf=confusion_matrix(true_class,predicted_class)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(conf, annot=True, ax = ax, fmt=\".1f\")\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Flooding', 'Impersination', 'Injection', 'Normal']); ax.yaxis.set_ticklabels(['Flooding', 'Impersination', 'Injection', 'Normal']);\n",
    "    plt.show()\n",
    "\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IM",
   "language": "python",
   "name": "im"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
