{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install / upgrade necessary packages\n",
    "# !pip install easydict\n",
    "# !pip install numpy\n",
    "# !pip install numpy --upgrade # upgrade numpy to at least 1.19\n",
    "# !pip install torch --upgrade # upgrade pytorch to 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict()\n",
    "\n",
    "args.epochs=100\n",
    "args.dropout=0.0\n",
    "args.lr=5e-5\n",
    "args.gamma_exp=1000\n",
    "args.hidden=800\n",
    "args.ratio=1\n",
    "args.dis_lambda=1.0\n",
    "args.lambda_m=0.0\n",
    "args.wgan='wgan'\n",
    "args.clamp_lower=-0.15\n",
    "args.clamp_upper=0.15\n",
    "args.batch_size=100\n",
    "args.num_train=100\n",
    "args.loss='default'\n",
    "args.evaluate=False\n",
    "args.checkpoint='none'\n",
    "args.save_head='tmp'\n",
    "args.save_interval=20\n",
    "args.log_interval=20\n",
    "args.log_file='tmp_mlp'\n",
    "args.seed=2 #2\n",
    "args.cuda=False\n",
    "args.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.18.5\n",
      "Pytorch version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from utils import plain_log\n",
    "from utils import write_pickle,read_pickle\n",
    "from utils import masked_cross_entropy\n",
    "from utils import gaussian_loss\n",
    "from torch.utils import data\n",
    "from data_loader import toydata\n",
    "\n",
    "label_noise_std = 0.20\n",
    "use_label_noise = False\n",
    "use_inverse_weighted = True\n",
    "discr_thres = 999.999\n",
    "normalize = True\n",
    "train_discr_step_tot = 2\n",
    "train_discr_step_extra = 0\n",
    "slow_lrD_decay = 1\n",
    "norm = 8\n",
    "fname_save = 'pred_tmp.pkl'\n",
    "fname = '/Users/avin/Downloads/CIDA-master/toy-circle/awid.pkl'\n",
    "train_list = list(range(2))\n",
    "mask_list = [1] * 5 + [0] * 7\n",
    "test_list = list(range(2))\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(int(args.seed))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname, train_list, normalize, mask_list),\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    toydata(fname, test_list, normalize),\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pytorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x324 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load raw data and plot it for visualization only\n",
    "from plot import plot_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "data_pkl = read_pickle('/Users/avin/Downloads/CIDA-master/toy-circle/awid.pkl')\n",
    "plot_dataset(data_pkl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: domain as input to generate feature, and then concatenate, deep dynamic layers\n",
    "class DomainEnc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainEnc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.ratio = float(args.ratio)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.fc1 = nn.Linear(153, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop3 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4 = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden + int(self.hidden // self.ratio))\n",
    "        self.drop4 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden + int(self.hidden // self.ratio), self.hidden)\n",
    "\n",
    "        self.fc1_var = nn.Linear(1, int(self.hidden // self.ratio))\n",
    "        self.fc2_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.fc3_var = nn.Linear(int(self.hidden // self.ratio), int(self.hidden // self.ratio))\n",
    "        self.drop1_var = nn.Dropout(self.dropout)\n",
    "        self.drop2_var = nn.Dropout(self.dropout)\n",
    "        self.drop3_var = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        # side branch for variable FC\n",
    "        x_domain = F.relu(self.fc1_var(domain))\n",
    "        x_domain = self.drop1_var(x_domain)\n",
    "\n",
    "        # main branch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # combine feature in the middle\n",
    "        x = torch.cat((x, x_domain), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        # continue main branch\n",
    "        x = F.relu(self.fc_final(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor\n",
    "class DomainPred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainPred, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop0 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden, self.hidden)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "        self.fc_final = nn.Linear(self.hidden, 153)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop0(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc_final(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator: with BN layers after each FC, dual output\n",
    "class DomainDDisc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainDDisc, self).__init__()\n",
    "        self.hidden = args.hidden\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc3_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn3_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop3_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc4_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn4_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop4_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc5_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn5_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop5_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc6_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn6_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop6_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_m = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_m = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_m = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc7_s = nn.Linear(self.hidden, self.hidden)\n",
    "        self.bn7_s = nn.BatchNorm1d(self.hidden)\n",
    "        self.drop7_s = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_final_m = nn.Linear(self.hidden, 1)\n",
    "        self.fc_final_s = nn.Linear(self.hidden, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, domain = x\n",
    "        domain = domain.unsqueeze(1) / norm\n",
    "\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x_m = F.relu(self.bn3_m(self.fc3_m(x)))\n",
    "        x_m = self.drop3_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn3_s(self.fc3_s(x)))\n",
    "        x_s = self.drop3_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn4_m(self.fc4_m(x_m)))\n",
    "        x_m = self.drop4_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn4_s(self.fc4_s(x_s)))\n",
    "        x_s = self.drop4_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn5_m(self.fc5_m(x_m)))\n",
    "        x_m = self.drop5_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn5_s(self.fc5_s(x_s)))\n",
    "        x_s = self.drop5_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn6_m(self.fc6_m(x_m)))\n",
    "        x_m = self.drop6_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn6_s(self.fc6_s(x_s)))\n",
    "        x_s = self.drop6_s(x_s)\n",
    "\n",
    "        x_m = F.relu(self.bn7_m(self.fc7_m(x_m)))\n",
    "        x_m = self.drop7_m(x_m)\n",
    "\n",
    "        x_s = F.relu(self.bn7_s(self.fc7_s(x_s)))\n",
    "        x_s = self.drop7_s(x_s)\n",
    "\n",
    "        x_m = self.fc_final_m(x_m)\n",
    "        x_s = self.fc_final_s(x_s) # log sigma^2\n",
    "\n",
    "        return (x_m, x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "encoder = DomainEnc()\n",
    "predictor = DomainPred()\n",
    "discriminator = DomainDDisc()\n",
    "models = [encoder, predictor, discriminator]\n",
    "if args.cuda:\n",
    "    for model in models:\n",
    "        model.cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: avg_discr_loss = 0.05417, avg_pred_loss = 5497918566.400, avg_total_loss = 5497918566.400\n",
      "Test set: Average loss: 3384902462.1226668, Accuracy: 2359/3000 (78.63%)\n",
      "Train Epoch 2: avg_discr_loss = -0.09936, avg_pred_loss = 2275094240.000, avg_total_loss = 2275094240.000\n",
      "Test set: Average loss: 2259282438.8266668, Accuracy: 2575/3000 (85.83%)\n",
      "Train Epoch 3: avg_discr_loss = -0.20874, avg_pred_loss = 1523757864.533, avg_total_loss = 1523757864.533\n",
      "Test set: Average loss: 786645886.2933333, Accuracy: 2743/3000 (91.43%)\n",
      "Train Epoch 4: avg_discr_loss = -0.31047, avg_pred_loss = 1135570520.267, avg_total_loss = 1135570520.267\n",
      "Test set: Average loss: 793660798.2933333, Accuracy: 2805/3000 (93.50%)\n",
      "Train Epoch 5: avg_discr_loss = -0.40543, avg_pred_loss = 1310628496.000, avg_total_loss = 1310628496.000\n",
      "Test set: Average loss: 1796859816.6186666, Accuracy: 2652/3000 (88.40%)\n",
      "Train Epoch 6: avg_discr_loss = -0.50250, avg_pred_loss = 1241009707.733, avg_total_loss = 1241009707.733\n",
      "Test set: Average loss: 1015416545.2800000, Accuracy: 2804/3000 (93.47%)\n",
      "Train Epoch 7: avg_discr_loss = -0.60579, avg_pred_loss = 812658305.600, avg_total_loss = 812658305.600\n",
      "Test set: Average loss: 850076953.2586666, Accuracy: 2778/3000 (92.60%)\n",
      "Train Epoch 8: avg_discr_loss = -0.69942, avg_pred_loss = 1276611608.267, avg_total_loss = 1276611608.267\n",
      "Test set: Average loss: 1038820037.9733334, Accuracy: 2775/3000 (92.50%)\n",
      "Train Epoch 9: avg_discr_loss = -0.78189, avg_pred_loss = 1262310467.733, avg_total_loss = 1262310467.733\n",
      "Test set: Average loss: 397648776.5333334, Accuracy: 2846/3000 (94.87%)\n",
      "Train Epoch 10: avg_discr_loss = -0.84705, avg_pred_loss = 508569905.333, avg_total_loss = 508569905.359\n",
      "Test set: Average loss: 936324024.3200001, Accuracy: 2315/3000 (77.17%)\n",
      "Train Epoch 11: avg_discr_loss = -0.88696, avg_pred_loss = 516999054.133, avg_total_loss = 516999054.133\n",
      "Test set: Average loss: 307072849.2373334, Accuracy: 2847/3000 (94.90%)\n",
      "Train Epoch 12: avg_discr_loss = -0.90622, avg_pred_loss = 479327645.867, avg_total_loss = 479327645.867\n",
      "Test set: Average loss: 396534534.1440000, Accuracy: 2817/3000 (93.90%)\n",
      "Train Epoch 13: avg_discr_loss = -0.91240, avg_pred_loss = 481416652.533, avg_total_loss = 481416652.533\n",
      "Test set: Average loss: 688747332.9493333, Accuracy: 2783/3000 (92.77%)\n",
      "Train Epoch 14: avg_discr_loss = -0.91431, avg_pred_loss = 492577779.733, avg_total_loss = 492577779.733\n",
      "Test set: Average loss: 411668199.7653334, Accuracy: 2835/3000 (94.50%)\n",
      "Train Epoch 15: avg_discr_loss = -0.91577, avg_pred_loss = 446944534.933, avg_total_loss = 446944534.933\n",
      "Test set: Average loss: 319749622.7840000, Accuracy: 2718/3000 (90.60%)\n",
      "Train Epoch 16: avg_discr_loss = -0.91618, avg_pred_loss = 405484514.267, avg_total_loss = 405484514.267\n",
      "Test set: Average loss: 516299523.4133334, Accuracy: 2693/3000 (89.77%)\n",
      "Train Epoch 17: avg_discr_loss = -0.91554, avg_pred_loss = 500271725.867, avg_total_loss = 500271725.867\n",
      "Test set: Average loss: 313639036.5866666, Accuracy: 2808/3000 (93.60%)\n",
      "Train Epoch 18: avg_discr_loss = -0.91582, avg_pred_loss = 535535393.333, avg_total_loss = 535535393.333\n",
      "Test set: Average loss: 601724112.5546666, Accuracy: 2839/3000 (94.63%)\n",
      "Train Epoch 19: avg_discr_loss = -0.91613, avg_pred_loss = 366485747.067, avg_total_loss = 366485747.067\n",
      "Test set: Average loss: 306241581.0560000, Accuracy: 2835/3000 (94.50%)\n",
      "Train Epoch 20: avg_discr_loss = -0.91594, avg_pred_loss = 310509461.525, avg_total_loss = 310509461.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avin/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type DomainEnc. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "/Users/avin/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type DomainPred. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "/Users/avin/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type DomainDDisc. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 287103896.5760000, Accuracy: 2683/3000 (89.43%)\n",
      "Train Epoch 21: avg_discr_loss = -0.91560, avg_pred_loss = 333134561.200, avg_total_loss = 333134561.267\n",
      "Test set: Average loss: 261216972.4586667, Accuracy: 2748/3000 (91.60%)\n",
      "Train Epoch 22: avg_discr_loss = -0.91612, avg_pred_loss = 334918629.467, avg_total_loss = 334918629.467\n",
      "Test set: Average loss: 263436359.3386667, Accuracy: 2306/3000 (76.87%)\n",
      "Train Epoch 23: avg_discr_loss = -0.91625, avg_pred_loss = 324171017.333, avg_total_loss = 324171017.333\n",
      "Test set: Average loss: 334909768.3626667, Accuracy: 2713/3000 (90.43%)\n",
      "Train Epoch 24: avg_discr_loss = -0.91734, avg_pred_loss = 222032704.067, avg_total_loss = 222032704.200\n",
      "Test set: Average loss: 236406854.9973333, Accuracy: 2834/3000 (94.47%)\n",
      "Train Epoch 25: avg_discr_loss = -0.91730, avg_pred_loss = 308639040.800, avg_total_loss = 308639040.833\n",
      "Test set: Average loss: 314720290.1333333, Accuracy: 2838/3000 (94.60%)\n",
      "Train Epoch 26: avg_discr_loss = -0.91731, avg_pred_loss = 260509282.133, avg_total_loss = 260509282.133\n",
      "Test set: Average loss: 189354541.0560000, Accuracy: 2466/3000 (82.20%)\n",
      "Train Epoch 27: avg_discr_loss = -0.91701, avg_pred_loss = 192118631.800, avg_total_loss = 192118631.867\n",
      "Test set: Average loss: 198902177.7920000, Accuracy: 2847/3000 (94.90%)\n",
      "Train Epoch 28: avg_discr_loss = -0.91707, avg_pred_loss = 235852090.667, avg_total_loss = 235852090.700\n",
      "Test set: Average loss: 355986526.2080000, Accuracy: 2773/3000 (92.43%)\n",
      "Train Epoch 29: avg_discr_loss = -0.91751, avg_pred_loss = 198682611.933, avg_total_loss = 198682612.000\n",
      "Test set: Average loss: 240130775.0400000, Accuracy: 2814/3000 (93.80%)\n",
      "Train Epoch 30: avg_discr_loss = -0.91748, avg_pred_loss = 162685359.767, avg_total_loss = 162685359.933\n",
      "Test set: Average loss: 122264798.2080000, Accuracy: 2852/3000 (95.07%)\n",
      "Train Epoch 31: avg_discr_loss = -0.91744, avg_pred_loss = 151665769.667, avg_total_loss = 151665769.800\n",
      "Test set: Average loss: 129754279.4240000, Accuracy: 2698/3000 (89.93%)\n",
      "Train Epoch 32: avg_discr_loss = -0.91739, avg_pred_loss = 195339432.800, avg_total_loss = 195339432.967\n",
      "Test set: Average loss: 209760673.6213333, Accuracy: 2772/3000 (92.40%)\n",
      "Train Epoch 33: avg_discr_loss = -0.91725, avg_pred_loss = 161894020.733, avg_total_loss = 161894020.867\n",
      "Test set: Average loss: 170773807.1040000, Accuracy: 2828/3000 (94.27%)\n",
      "Train Epoch 34: avg_discr_loss = -0.91725, avg_pred_loss = 138059298.967, avg_total_loss = 138059299.133\n",
      "Test set: Average loss: 137798903.1253333, Accuracy: 2744/3000 (91.47%)\n",
      "Train Epoch 35: avg_discr_loss = -0.91726, avg_pred_loss = 172323676.575, avg_total_loss = 172323676.708\n",
      "Test set: Average loss: 132300825.0880000, Accuracy: 2658/3000 (88.60%)\n",
      "Train Epoch 36: avg_discr_loss = -0.91736, avg_pred_loss = 152826950.100, avg_total_loss = 152826950.200\n",
      "Test set: Average loss: 94920772.2666667, Accuracy: 2820/3000 (94.00%)\n",
      "Train Epoch 37: avg_discr_loss = -0.91762, avg_pred_loss = 126835404.600, avg_total_loss = 126835404.700\n",
      "Test set: Average loss: 109121131.3493333, Accuracy: 2661/3000 (88.70%)\n",
      "Train Epoch 38: avg_discr_loss = -0.91757, avg_pred_loss = 132991114.667, avg_total_loss = 132991114.733\n",
      "Test set: Average loss: 127685357.5680000, Accuracy: 2370/3000 (79.00%)\n",
      "Train Epoch 39: avg_discr_loss = -0.91744, avg_pred_loss = 130111358.700, avg_total_loss = 130111358.800\n",
      "Test set: Average loss: 71447346.8586667, Accuracy: 2375/3000 (79.17%)\n",
      "Train Epoch 40: avg_discr_loss = -0.91759, avg_pred_loss = 130975992.600, avg_total_loss = 130975992.667\n",
      "Test set: Average loss: 140757001.0453333, Accuracy: 2836/3000 (94.53%)\n",
      "Train Epoch 41: avg_discr_loss = -0.91756, avg_pred_loss = 136147311.100, avg_total_loss = 136147311.333\n",
      "Test set: Average loss: 172650472.7893333, Accuracy: 2595/3000 (86.50%)\n",
      "Train Epoch 42: avg_discr_loss = -0.91773, avg_pred_loss = 327335730.367, avg_total_loss = 327335730.433\n",
      "Test set: Average loss: 207038035.7973333, Accuracy: 2298/3000 (76.60%)\n",
      "Train Epoch 43: avg_discr_loss = -0.91723, avg_pred_loss = 208808956.000, avg_total_loss = 208808956.067\n",
      "Test set: Average loss: 114949680.1280000, Accuracy: 2809/3000 (93.63%)\n",
      "Train Epoch 44: avg_discr_loss = -0.91723, avg_pred_loss = 190502114.533, avg_total_loss = 190502114.533\n",
      "Test set: Average loss: 109104131.2426667, Accuracy: 2716/3000 (90.53%)\n",
      "Train Epoch 45: avg_discr_loss = -0.91758, avg_pred_loss = 202472731.867, avg_total_loss = 202472731.867\n",
      "Test set: Average loss: 184766475.4346667, Accuracy: 2802/3000 (93.40%)\n",
      "Train Epoch 46: avg_discr_loss = -0.91767, avg_pred_loss = 143193802.933, avg_total_loss = 143193803.067\n",
      "Test set: Average loss: 131307520.0000000, Accuracy: 2822/3000 (94.07%)\n",
      "Train Epoch 47: avg_discr_loss = -0.91767, avg_pred_loss = 103105183.000, avg_total_loss = 103105183.200\n",
      "Test set: Average loss: 55760327.3386667, Accuracy: 2842/3000 (94.73%)\n",
      "Train Epoch 48: avg_discr_loss = -0.91766, avg_pred_loss = 130165108.267, avg_total_loss = 130165108.400\n",
      "Test set: Average loss: 83447165.4400000, Accuracy: 2813/3000 (93.77%)\n",
      "Train Epoch 49: avg_discr_loss = -0.91739, avg_pred_loss = 81038468.483, avg_total_loss = 81038468.850\n",
      "Test set: Average loss: 83468379.1360000, Accuracy: 2848/3000 (94.93%)\n",
      "Train Epoch 50: avg_discr_loss = -0.91783, avg_pred_loss = 88071574.133, avg_total_loss = 88071574.367\n",
      "Test set: Average loss: 82326602.0693333, Accuracy: 2847/3000 (94.90%)\n",
      "Train Epoch 51: avg_discr_loss = -0.91789, avg_pred_loss = 88164050.117, avg_total_loss = 88164050.350\n",
      "Test set: Average loss: 77774485.3333333, Accuracy: 2848/3000 (94.93%)\n",
      "Train Epoch 52: avg_discr_loss = -0.91787, avg_pred_loss = 108205488.650, avg_total_loss = 108205488.883\n",
      "Test set: Average loss: 90489159.3386667, Accuracy: 2817/3000 (93.90%)\n",
      "Train Epoch 53: avg_discr_loss = -0.91761, avg_pred_loss = 55857576.467, avg_total_loss = 55857577.000\n",
      "Test set: Average loss: 47159235.4133333, Accuracy: 2561/3000 (85.37%)\n",
      "Train Epoch 54: avg_discr_loss = -0.91781, avg_pred_loss = 78683664.133, avg_total_loss = 78683664.633\n",
      "Test set: Average loss: 127064136.0213333, Accuracy: 2685/3000 (89.50%)\n",
      "Train Epoch 55: avg_discr_loss = -0.91779, avg_pred_loss = 92849310.933, avg_total_loss = 92849311.000\n",
      "Test set: Average loss: 66796436.1386667, Accuracy: 2802/3000 (93.40%)\n",
      "Train Epoch 56: avg_discr_loss = -0.91780, avg_pred_loss = 60036452.183, avg_total_loss = 60036452.650\n",
      "Test set: Average loss: 83153857.2800000, Accuracy: 2673/3000 (89.10%)\n",
      "Train Epoch 57: avg_discr_loss = -0.91778, avg_pred_loss = 61481369.950, avg_total_loss = 61481370.383\n",
      "Test set: Average loss: 88642828.3733333, Accuracy: 2685/3000 (89.50%)\n",
      "Train Epoch 58: avg_discr_loss = -0.91772, avg_pred_loss = 69897758.667, avg_total_loss = 69897759.167\n",
      "Test set: Average loss: 71351666.6880000, Accuracy: 2435/3000 (81.17%)\n",
      "Train Epoch 59: avg_discr_loss = -0.91778, avg_pred_loss = 49552549.633, avg_total_loss = 49552550.300\n",
      "Test set: Average loss: 37846238.0373333, Accuracy: 2824/3000 (94.13%)\n",
      "Train Epoch 60: avg_discr_loss = -0.91790, avg_pred_loss = 56129776.300, avg_total_loss = 56129776.800\n",
      "Test set: Average loss: 124265612.9706667, Accuracy: 2587/3000 (86.23%)\n",
      "Train Epoch 61: avg_discr_loss = -0.91771, avg_pred_loss = 66038656.667, avg_total_loss = 66038656.967\n",
      "Test set: Average loss: 47326737.4933333, Accuracy: 2821/3000 (94.03%)\n",
      "Train Epoch 62: avg_discr_loss = -0.91793, avg_pred_loss = 56298431.067, avg_total_loss = 56298431.500\n",
      "Test set: Average loss: 45326897.4080000, Accuracy: 2852/3000 (95.07%)\n",
      "Train Epoch 63: avg_discr_loss = -0.91789, avg_pred_loss = 66361433.833, avg_total_loss = 66361434.267\n",
      "Test set: Average loss: 46376344.7466667, Accuracy: 2813/3000 (93.77%)\n",
      "Train Epoch 64: avg_discr_loss = -0.91784, avg_pred_loss = 54623961.267, avg_total_loss = 54623961.800\n",
      "Test set: Average loss: 38253986.5600000, Accuracy: 2460/3000 (82.00%)\n",
      "Train Epoch 65: avg_discr_loss = -0.91793, avg_pred_loss = 55031084.100, avg_total_loss = 55031084.500\n",
      "Test set: Average loss: 46065806.0800000, Accuracy: 2849/3000 (94.97%)\n",
      "Train Epoch 66: avg_discr_loss = -0.91778, avg_pred_loss = 67426327.833, avg_total_loss = 67426328.033\n",
      "Test set: Average loss: 72846653.0986667, Accuracy: 2696/3000 (89.87%)\n",
      "Train Epoch 67: avg_discr_loss = -0.91787, avg_pred_loss = 58841396.567, avg_total_loss = 58841397.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 75013794.9866667, Accuracy: 2539/3000 (84.63%)\n",
      "Train Epoch 68: avg_discr_loss = -0.91771, avg_pred_loss = 61008273.783, avg_total_loss = 61008274.150\n",
      "Test set: Average loss: 66648582.0586667, Accuracy: 2826/3000 (94.20%)\n",
      "Train Epoch 69: avg_discr_loss = -0.91783, avg_pred_loss = 62765582.333, avg_total_loss = 62765582.701\n",
      "Test set: Average loss: 46733631.1466667, Accuracy: 2809/3000 (93.63%)\n",
      "Train Epoch 70: avg_discr_loss = -0.91791, avg_pred_loss = 56335820.367, avg_total_loss = 56335820.833\n",
      "Test set: Average loss: 63972714.3253333, Accuracy: 2850/3000 (95.00%)\n",
      "Train Epoch 71: avg_discr_loss = -0.91791, avg_pred_loss = 69722471.400, avg_total_loss = 69722471.667\n",
      "Test set: Average loss: 61648808.2773333, Accuracy: 2846/3000 (94.87%)\n",
      "Train Epoch 72: avg_discr_loss = -0.91768, avg_pred_loss = 65207730.667, avg_total_loss = 65207731.067\n",
      "Test set: Average loss: 51886067.8826667, Accuracy: 2815/3000 (93.83%)\n",
      "Train Epoch 73: avg_discr_loss = -0.91779, avg_pred_loss = 66597799.200, avg_total_loss = 66597799.467\n",
      "Test set: Average loss: 85396384.2560000, Accuracy: 2820/3000 (94.00%)\n",
      "Train Epoch 74: avg_discr_loss = -0.91801, avg_pred_loss = 61631624.150, avg_total_loss = 61631624.550\n",
      "Test set: Average loss: 48864062.2933333, Accuracy: 2819/3000 (93.97%)\n",
      "Train Epoch 75: avg_discr_loss = -0.91776, avg_pred_loss = 43465044.100, avg_total_loss = 43465044.733\n",
      "Test set: Average loss: 38794799.4453333, Accuracy: 2693/3000 (89.77%)\n",
      "Train Epoch 76: avg_discr_loss = -0.91769, avg_pred_loss = 40016738.883, avg_total_loss = 40016739.483\n",
      "Test set: Average loss: 29699960.8320000, Accuracy: 2846/3000 (94.87%)\n",
      "Train Epoch 77: avg_discr_loss = -0.91733, avg_pred_loss = 46015398.033, avg_total_loss = 46015398.467\n",
      "Test set: Average loss: 36145620.9920000, Accuracy: 2356/3000 (78.53%)\n",
      "Train Epoch 78: avg_discr_loss = -0.91772, avg_pred_loss = 26689263.450, avg_total_loss = 26689264.650\n",
      "Test set: Average loss: 44271461.8880000, Accuracy: 2792/3000 (93.07%)\n",
      "Train Epoch 79: avg_discr_loss = -0.91774, avg_pred_loss = 35361578.587, avg_total_loss = 35361579.421\n",
      "Test set: Average loss: 24838101.5893333, Accuracy: 2819/3000 (93.97%)\n",
      "Train Epoch 80: avg_discr_loss = -0.91714, avg_pred_loss = 33481288.933, avg_total_loss = 33481289.700\n",
      "Test set: Average loss: 21845219.9253333, Accuracy: 2852/3000 (95.07%)\n",
      "Train Epoch 81: avg_discr_loss = -0.91781, avg_pred_loss = 37251366.967, avg_total_loss = 37251367.900\n",
      "Test set: Average loss: 41113438.8906667, Accuracy: 2816/3000 (93.87%)\n",
      "Train Epoch 82: avg_discr_loss = -0.91763, avg_pred_loss = 34270008.700, avg_total_loss = 34270009.833\n",
      "Test set: Average loss: 48372871.7653333, Accuracy: 2836/3000 (94.53%)\n",
      "Train Epoch 83: avg_discr_loss = -0.91783, avg_pred_loss = 43390400.567, avg_total_loss = 43390401.300\n",
      "Test set: Average loss: 28416660.9920000, Accuracy: 2840/3000 (94.67%)\n",
      "Train Epoch 84: avg_discr_loss = -0.91771, avg_pred_loss = 34089962.183, avg_total_loss = 34089963.283\n",
      "Test set: Average loss: 21833980.5866667, Accuracy: 2761/3000 (92.03%)\n",
      "Train Epoch 85: avg_discr_loss = -0.91784, avg_pred_loss = 34191592.750, avg_total_loss = 34191593.683\n",
      "Test set: Average loss: 35945020.6720000, Accuracy: 2726/3000 (90.87%)\n",
      "Train Epoch 86: avg_discr_loss = -0.91780, avg_pred_loss = 33902755.033, avg_total_loss = 33902756.134\n",
      "Test set: Average loss: 27455441.7493333, Accuracy: 2842/3000 (94.73%)\n",
      "Train Epoch 87: avg_discr_loss = -0.91778, avg_pred_loss = 49635070.850, avg_total_loss = 49635071.517\n",
      "Test set: Average loss: 12153272.2346667, Accuracy: 2671/3000 (89.03%)\n",
      "Train Epoch 88: avg_discr_loss = -0.91789, avg_pred_loss = 45903294.058, avg_total_loss = 45903294.725\n",
      "Test set: Average loss: 30346065.2373333, Accuracy: 2816/3000 (93.87%)\n",
      "Train Epoch 89: avg_discr_loss = -0.91797, avg_pred_loss = 39355827.283, avg_total_loss = 39355827.884\n",
      "Test set: Average loss: 71963193.0880000, Accuracy: 2688/3000 (89.60%)\n",
      "Train Epoch 90: avg_discr_loss = -0.91801, avg_pred_loss = 33790687.700, avg_total_loss = 33790688.633\n",
      "Test set: Average loss: 25795797.6746667, Accuracy: 2558/3000 (85.27%)\n",
      "Train Epoch 91: avg_discr_loss = -0.91801, avg_pred_loss = 36042729.483, avg_total_loss = 36042730.350\n",
      "Test set: Average loss: 68825810.6026667, Accuracy: 2211/3000 (73.70%)\n",
      "Train Epoch 92: avg_discr_loss = -0.91790, avg_pred_loss = 41084535.883, avg_total_loss = 41084536.783\n",
      "Test set: Average loss: 41254626.1333333, Accuracy: 2823/3000 (94.10%)\n",
      "Train Epoch 93: avg_discr_loss = -0.91792, avg_pred_loss = 29436908.658, avg_total_loss = 29436909.592\n",
      "Test set: Average loss: 34948610.0480000, Accuracy: 2843/3000 (94.77%)\n",
      "Train Epoch 94: avg_discr_loss = -0.91802, avg_pred_loss = 31811511.817, avg_total_loss = 31811512.650\n",
      "Test set: Average loss: 38411206.8266667, Accuracy: 2830/3000 (94.33%)\n",
      "Train Epoch 95: avg_discr_loss = -0.91792, avg_pred_loss = 32249654.017, avg_total_loss = 32249654.950\n",
      "Test set: Average loss: 23126776.5760000, Accuracy: 2822/3000 (94.07%)\n",
      "Train Epoch 96: avg_discr_loss = -0.91776, avg_pred_loss = 41109481.717, avg_total_loss = 41109482.517\n",
      "Test set: Average loss: 40467656.0213333, Accuracy: 2715/3000 (90.50%)\n",
      "Train Epoch 97: avg_discr_loss = -0.91775, avg_pred_loss = 28098862.292, avg_total_loss = 28098863.358\n",
      "Test set: Average loss: 23046034.6026667, Accuracy: 2851/3000 (95.03%)\n",
      "Train Epoch 98: avg_discr_loss = -0.91798, avg_pred_loss = 27117890.900, avg_total_loss = 27117892.167\n",
      "Test set: Average loss: 34856764.5013333, Accuracy: 2792/3000 (93.07%)\n",
      "Train Epoch 99: avg_discr_loss = -0.91793, avg_pred_loss = 34595636.129, avg_total_loss = 34595636.996\n",
      "Test set: Average loss: 32542757.9733333, Accuracy: 2824/3000 (94.13%)\n",
      "Train Epoch 100: avg_discr_loss = -0.91744, avg_pred_loss = 23307644.567, avg_total_loss = 23307646.000\n",
      "Test set: Average loss: 10106019.7546667, Accuracy: 2845/3000 (94.83%)\n",
      "Test set: Average loss: 10106019.7546667, Accuracy: 2845/3000 (94.83%)\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set up optimizer\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "\n",
    "ind = list(range(args.batch_size))\n",
    "ind_test = list(range(1000))\n",
    "bce = nn.BCELoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    sum_discr_loss = 0\n",
    "    sum_total_loss = 0\n",
    "    sum_pred_loss = 0\n",
    "    for batch_idx, data_tuple in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data, mask = data_tuple\n",
    "        else:\n",
    "            data, target, domain, mask = data_tuple\n",
    "\n",
    "        # FF encoder and predictor\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "\n",
    "        if use_label_noise:\n",
    "            noise = (torch.randn(domain.size()).cuda() * label_noise_std).unsqueeze(1)\n",
    "\n",
    "        # train discriminator\n",
    "        train_discr_step = 0\n",
    "        while args.dis_lambda > 0.0:\n",
    "            train_discr_step += 1\n",
    "            discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "            discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm, np.mean(train_list) / norm, norm)\n",
    "            for model in models:\n",
    "                model.zero_grad()\n",
    "            discr_loss.backward(retain_graph=True)\n",
    "            opt_D.step()\n",
    "\n",
    "            # handle extra steps to train the discr's variance branch\n",
    "            if train_discr_step_extra > 0:\n",
    "                cur_extra_step = 0\n",
    "                while True:\n",
    "                    discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "                    discr_loss = gaussian_loss(discr_pred_m.detach(), discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "                    for model in models:\n",
    "                        model.zero_grad()\n",
    "                    discr_loss.backward(retain_graph=True)\n",
    "                    opt_D.step()\n",
    "                    cur_extra_step += 1\n",
    "                    if cur_extra_step > train_discr_step_extra:\n",
    "                        break\n",
    "\n",
    "            if discr_loss.item() < 1.1 * discr_thres and train_discr_step >= train_discr_step_tot:\n",
    "                sum_discr_loss += discr_loss.item()\n",
    "                break\n",
    "\n",
    "        # handle wgan\n",
    "        if args.wgan == 'wgan':\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(args.clamp_lower, args.clamp_upper)\n",
    "\n",
    "        # train encoder and predictor\n",
    "        pred_loss = masked_cross_entropy(prediction, target, mask)\n",
    "        discr_pred_m, discr_pred_s = discriminator((encoding, domain))\n",
    "        ent_loss = 0\n",
    "\n",
    "        discr_loss = gaussian_loss(discr_pred_m, discr_pred_s, domain.unsqueeze(1) / norm)\n",
    "        total_loss = pred_loss - discr_loss * args.dis_lambda\n",
    "\n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt_non_D.step()\n",
    "        sum_pred_loss += pred_loss.item()\n",
    "        sum_total_loss += total_loss.item()\n",
    "\n",
    "    lr_scheduler_D.step()\n",
    "    lr_scheduler_non_D.step()\n",
    "\n",
    "    avg_discr_loss = sum_discr_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_pred_loss = sum_pred_loss / len(train_loader.dataset) * args.batch_size\n",
    "    avg_total_loss = sum_total_loss / len(train_loader.dataset) * args.batch_size\n",
    "    log_txt = 'Train Epoch {}: avg_discr_loss = {:.5f}, avg_pred_loss = {:.3f}, avg_total_loss = {:.3f}'.format(epoch, avg_discr_loss, avg_pred_loss, avg_total_loss)\n",
    "    print(log_txt)\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "    if epoch % args.save_interval == 0 and epoch != 0:\n",
    "        torch.save(encoder, '%s.model_enc' % args.save_head)\n",
    "        torch.save(predictor, '%s.model_pred' % args.save_head)\n",
    "        torch.save(discriminator, '%s.model_discr' % args.save_head)\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    test_loss = 0\n",
    "    rmse_loss = 0\n",
    "    correct = 0\n",
    "    l_data = []\n",
    "    l_label = []\n",
    "    l_gt = []\n",
    "    l_encoding = []\n",
    "    l_domain = []\n",
    "    l_prob = []\n",
    "    #for data, target, domain in test_loader:\n",
    "    for data_tuple in test_loader:\n",
    "        if args.cuda:\n",
    "            data_tuple = tuple(ele.cuda() for ele in data_tuple)\n",
    "        if normalize:\n",
    "            data_raw, target, domain, data = data_tuple\n",
    "        else:\n",
    "            data, target, domain = data_tuple\n",
    "            data_raw = data\n",
    "        encoding = encoder((data, domain))\n",
    "        prediction = predictor((encoding, domain))\n",
    "        test_loss += F.nll_loss(prediction, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = prediction.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "\n",
    "        l_data.append(data_raw.cpu().numpy())\n",
    "        l_label.append(pred.cpu().numpy())\n",
    "        l_gt.append(target.cpu().numpy())\n",
    "        l_encoding.append(encoding.data.cpu().numpy())\n",
    "        l_domain.append(domain.data.cpu().numpy())\n",
    "        l_prob.append(prediction.data.cpu().numpy())\n",
    "\n",
    "    data_all = np.concatenate(l_data, axis=0)\n",
    "    label_all = np.concatenate(l_label, axis=0)\n",
    "    gt_all = np.concatenate(l_gt, axis=0)\n",
    "    encoding_all = np.concatenate(l_encoding, axis=0)\n",
    "    domain_all = np.concatenate(l_domain, axis=0)\n",
    "    prob_all = np.concatenate(l_prob, axis=0)\n",
    "    d_pkl = dict()\n",
    "    d_pkl['data'] = data_all\n",
    "    d_pkl['label'] = label_all[:, 0]\n",
    "    d_pkl['gt'] = gt_all\n",
    "    d_pkl['encoding'] = encoding_all\n",
    "    d_pkl['domain'] = domain_all\n",
    "    d_pkl['prob'] = prob_all\n",
    "    write_pickle(d_pkl, fname_save)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    log_txt = 'Test set: Average loss: {:.7f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset))\n",
    "    print(log_txt)\n",
    "    plain_log(args.log_file,log_txt+'\\n')\n",
    "\n",
    "if args.checkpoint != 'none':\n",
    "    encoder = torch.load(args.checkpoint + '_enc')\n",
    "    predictor = torch.load(args.checkpoint + '_pred')\n",
    "    discriminator = torch.load(args.checkpoint + '_discr')\n",
    "    opt_D = optim.Adam(discriminator.parameters(), lr = args.lr) # lr \n",
    "    opt_non_D = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr = args.lr) # lr \n",
    "    lr_scheduler_D = lr_scheduler.ExponentialLR(optimizer=opt_D, gamma=0.5 ** (1/(args.gamma_exp*(train_discr_step_extra+1)) * slow_lrD_decay))\n",
    "    lr_scheduler_non_D = lr_scheduler.ExponentialLR(optimizer=opt_non_D, gamma=0.5 ** (1/args.gamma_exp))\n",
    "    models = [encoder, predictor, discriminator]\n",
    "    for model in models:\n",
    "        for key, module in model._modules.items():\n",
    "            print('key', key)\n",
    "            print('module', module)\n",
    "\n",
    "if not args.evaluate:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch % 1 == 0:\n",
    "            test()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 153)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the model result\n",
    "from plot import plot_data_and_label\n",
    "info = read_pickle('pred_tmp.pkl')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6 * 0.75))\n",
    "print(info['data'].shape)\n",
    "plot_data_and_label(ax, info['data'], info['label'])\n",
    "plt.title(f\"Model's prediction: CIDA\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
